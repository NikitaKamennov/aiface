# Web UI для Ollama
# нагенеренно с помощью qwen-coder 7b и 14b с минимальным вмешательством кожаного

Веб-интерфейс для удобной работы с Ollama сервером. Проект разработан для упрощения взаимодействия с языковыми моделями через браузер.

## Описание

Это веб-оболочка для работы с Ollama сервером, которая позволяет удобно взаимодействовать с языковыми моделями через браузер. Проект оптимизирован для работы в различных операционных системах,
с основным фокусом на Windows, где модели лучше всего запускать на мощных игровых ПК.

## Функциональные возможности

### Управление моделями
- Добавление новых моделей
- Удаление существующих моделей
- Переключение между моделями
- Отображение информации о текущей модели

### Работа с контекстом
- Сохранение контекста беседы
- Загрузка сохраненного контекста
- Сброс контекста
- Визуальный индикатор размера контекста
- Автоматическое управление контекстом для оптимальной производительности

### Настройка параметров генерации
- Настройка коэффициента вариабельности

### Ролевая система
- Предустановленные роли с уникальными характеристиками
- Настраиваемые системные сообщения для каждой роли (ручками в проекте)
- Возможность создания пользовательских ролей (ручками в проекте)
- Быстрое переключение между ролями

## Инструкции по запуску

### Запуск через Docker

#### Первый запуск 
```./rerun.bat```
Этот скрипт:
- Создаст или перезапишет контейнер
- Настроит все необходимые параметры
- Запустит сервис

После запуска сервис будет доступен по адресу: `http://localhost:3003`

**Важно**: Используйте `rerun.bat` только при первом запуске или когда необходимо пересобрать контейнер с изменениями в проекте.

#### Последующие запуски

```bash ./run.bat```

Этот скрипт запустит ранее созданный контейнер с необходимыми параметрами.

## Системные требования

- Docker Desktop для Windows
- Рекомендуется мощный ПК для работы с языковыми моделями
- Порт 3003 должен быть свободен
- Ollama сервер должен быть доступен

## Примечания

Хотя проект оптимизирован для Windows, его легко адаптировать для работы на Unix-системах или macOS. Основные компоненты работают в контейнерах Docker, что обеспечивает отличную переносимость.

Запустить можно и без докера  просто в Vite на дев сервере если он мешается на стандартном порту можно в конфиге помянять порт для дев сервера Vite

Скачать сервер ollama и почитать про модели можно здесь `https://qwen-ai.com/2-5-coder/`

## Безопасность

Все взаимодействия происходят локально, данные не отправляются на внешние серверы. Конфиденциальность обеспечивается локальным выполнением всех операций.
